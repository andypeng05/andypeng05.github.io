---
title: "Home"
layout: splash
permalink: /
---

<!-- style -->
<link rel="stylesheet" href="/assets/css/styles.css">
<style>
  .page__content {
    font-size: 0.9em;
  }
  .page__content h2 {
    font-size: 1.3em;
  }
  .page__content p, .page__content li {
    font-size: 0.9em;
  }
  .initial-content {
    font-size: 0.9em;
  }
  .initial-content h2 {
    font-size: 1.3em;
  }
  table td p[style*="font-weight:700"] {
    font-size: 1.1em;
    margin-bottom: 0.3em;
    margin-top: 0;
    line-height: 1.2;
  }
  table {
    margin-bottom: 1.5em;
  }
  table td {
    padding: 20px;
  }
  table td br {
    line-height: 1.5;
  }
  .pub-image-cell {
    padding: 20px 0 20px 20px !important;
    width: 200px;
    vertical-align: middle;
  }
  .pub-text-cell {
    padding: 20px 20px 20px 40px !important;
    vertical-align: middle;
  }
  .pub-table {
    border-spacing: 0;
    border-collapse: separate;
  }
  .pub-text-cell a {
    color: white !important;
    text-decoration: none !important;
  }
  .pub-text-cell a:hover {
    text-decoration: underline !important;
  }
</style>

![Andy Peng](./images/andyprofile.jpg)
{:style="float: right; padding: 30px; max-width: 30%; min-width: 330px;"}

<br/>
Hi! I'm a 3rd year undergraduate Researcher and Engineer at [UC Berkeley](https://www.berkeley.edu) studying Electrical Engineering and Computer Science (EECS). 

I'm broadly interested in Reinforcement Learning and Robot Learning. 
Currently, I work on Policy Extraction and offline-to-online RL at [BAIR](https://bair.berkeley.edu), where I'm fortunate to be advised by professor [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/). I'm also grateful to work on Test-Time methods for RL as part of the inaugural OpenAI launchpad Fellows program. Previously, I've worked on RL post-training for VLAs with the [GEAR](https://research.nvidia.com/labs/gear/) team at NVIDIA, LLM Infra with the ChipNemo team at NVIDIA, and Energy Validation Infra for Robotaxi Charging at Tesla. Outside research, I build machine learning applications with my friends at [Cal Launchpad](https://launchpad.studentorg.berkeley.edu).

If you'd like to chat, please reach out at [andypeng at berkeley dot edu].

---

## Publications

<table class="pub-table" style="width:100%;border:0px;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td class="pub-image-cell">
        <img src='/images/paper-images/pld.png' width="200" alt="PLD Paper">
      </td>
      <td class="pub-text-cell">
        <p style="font-weight:700; font-size: 1.1em; margin-bottom: 0.3em; margin-top: 0; line-height: 1.2;">
        <strong>Self-improving Vision-Language-Action models with data generation via Residual RL</strong>
        </p>
        <a href="https://www.wenlixiao.com">Wenli Xiao*</a>,
        <a href="https://darthutopian.github.io">Haotian Lin*</a>,
        <span style="text-decoration: underline;">Andy Peng</span>,
        <a href="https://haoruxue.github.io/">Haoru Xue</a>,
        <a href="https://tairanhe.com/">Tairan He</a>,
        <a href="https://xieleo5.github.io/">Yuqi Xie</a>,
        <a href="https://sled.eecs.umich.edu/author/fengyuan-hu/">Fengyuan Hu</a>,
        <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>,
        <a href="https://zhengyiluo.com/">Zhengyi Luo</a>,
        <a href="https://jimfan.me/">Linxi "Jim" Fan</a>,
        <a href="https://www.gshi.me/">Guanya Shi</a>,
        <a href="https://yukezhu.me/">Yuke Zhu</a>,

        <br>
        <em>arXiv preprint</em>, 2025
        <br>
        [<a href="https://www.wenlixiao.com/self-improve-VLA-PLD/assets/doc/pld-fullpaper.pdf">paper</a>]
        [<a href="https://www.wenlixiao.com/self-improve-VLA-PLD">website</a>]
      </td>
    </tr>
  </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; margin-top: 1.5em;">
  <tbody>
    <tr>
      <td class="pub-image-cell">
        <img src='/images/paper-images/wsrl.pdf' width="200">
      </td>
      <td class="pub-text-cell">
        <p style="font-weight:700; font-size: 1.1em; margin-bottom: 0.3em; margin-top: 0; line-height: 1.2;">
        <strong>Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data</strong>
        </p>
        <a href="https://zhouzypaul.github.io">Zhiyuan Zhou*</a>,
        <span style="text-decoration: underline;">Andy Peng*</span>,
        <a href="https://colinqiyangli.github.io">Qiyang Li</a>,
        <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
        <a href="https://aviralkumar2907.github.io">Aviral Kumar</a>
        <br>
        <em>International Conference on Learning Representations (ICLR)</em>, 2025
        <br>
        [<a href="http://arxiv.org/abs/2412.07762">paper</a>]
        [<a href="https://zhouzypaul.github.io/wsrl/">website</a>]
        [<a href="https://github.com/zhouzypaul/wsrl">code</a>]
      </td>
    </tr>
  </tbody>
</table>


